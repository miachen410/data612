{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA 612 - Research Discussion Assignment 3\n",
    "\n",
    "##### As more systems and sectors are driven by predictive analytics, there is increasing awareness of the possibility and pitfalls of algorithmic discrimination. In what ways do you think Recommender Systems reinforce human bias? Reflecting on the techniques we have covered, do you think recommender systems reinforce or help to prevent unethical targeting or customer segmentation?  Please provide one or more examples to support your arguments.\n",
    "\n",
    "---\n",
    "\n",
    "Some recommender algorithms run the risk of replicating and even amplifying human biases, resulting in different treatment to various groups or a deliberately disparate impact on them. Below are three examples to support this argument:\n",
    "\n",
    "## Bias in Online Recruitment Tools\n",
    "\n",
    "Amazon recently discontinued use of a recruiting algorithm after discovering gender bias. As a company that has over 60% male workers where men hold 74% of managerial positions, the data that engineers used to create the algorithm were derived from resumes of white males. The algorithm was taught to recognize word patterns in the resumes, rather than relevant skill sets, and these data were benchmarked against the company's predominantly male engineering department to determine an applicant's fit. As a result, the AI software software favors male applicants over female applicants, and thus resulting in gender bias.\n",
    "\n",
    "## Bias in Facial Recognition Technology\n",
    "\n",
    "MIT researcher Joy Buolamwini found that the algorithms powering three commercially available facial recognition software systems were failing to recognize darker-skinned complexions. The accuracy for correctly identifying a white man as a male is about 99%, while it has 20%~34% error rate for wrongly identifying a darker-skinned female as a male. One of the causes is due to the lack of diversified representation in the data sets - most facial recognition training data sets are estimated to be more than 75% male and more than 80% white.\n",
    "\n",
    "## Bias in Criminal Justice Algorithms\n",
    "\n",
    "The COMPAS algorithm, which is used by judges to predict whether defendants should be detained or released on bail pending trail, was found to be biased against African-Americans. The algorithm assigns a risk score to a defendant's likelihood to commit a future offense, relying on the voluminous data available on arrest records, defendant demographics, and other variables. Compared to whites who were equally likely to re-offend, African-Americans were more likely to be assigned a higher risk score, resulting in longer periods of detention while awaiting trail. \n",
    "\n",
    "`source`: [\"Algorithmic bias detection and mitigation: Best practices and policies to reduce consumer harms\"](https://www.brookings.edu/research/algorithmic-bias-detection-and-mitigation-best-practices-and-policies-to-reduce-consumer-harms/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
