{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA 612 - Research Discussion 4\n",
    "\n",
    "##### Mitigating the Harm of Recommender Systems\n",
    "\n",
    "Read one or more of the articles below and consider how to counter the radicalizing effects of recommender systems or ways to prevent algorithmic discrimination.\n",
    "\n",
    "- Renee Diresta, Wired.com (2018): Up Next: A Better Recommendation System\n",
    "\n",
    "- Zeynep Tufekci, The New York Times (2018): YouTube, the Great Radicalizer\n",
    "\n",
    "- Sanjay Krishnan, Jay Patel, Michael J. Franklin, Ken Goldberg (n/a): Social Influence Bias in Recommender Systems: A Methodology for Learning, Analyzing, and Mitigating Bias in Ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## NYU's AI Now Institue\n",
    "\n",
    "New York University's AI Now Institue introduced a model framwork for governmental entities to use to create algorithmic impact assessments (AIAs), which evaluate the potential detrimental effects of an algorithm. AIA encompasses multiple rounds of review from internal, external, and public audiences. First, it assumes that after this review, a company will develop a list of potential harms or biases in their self-assessment. Second, if bias appears to have occurred, the AIA pushes for notice to be given to impacted populations and a comment period opened for response. And finally, the AIA process looks to federal and other entities to support user's right to challenge alglrithmic decisions that feel unfair.\n",
    "\n",
    "## Diversity in Design\n",
    "\n",
    "Employing diversity in the design of alforithms upfront will trigger and potentially avoid harmful discriminatory effects on certain protected groups, especially racial and ethnic minorities. Diversity should play a critical role in the work teams, training data and the level of cultural sensitivity within their decision-making processes. \n",
    "\n",
    "## Self-Regulatory Audit for Bias\n",
    "\n",
    "The formal and regular auditing of algorithms to check for bias is another best practice for detecting and mitigating bias. Audits prompt the review of both input data and output decisions, and when done by a third party evaluator, they can provide insight into the algorithm's behavior. Human involvement in the process can also help to idenfity and correct biased outcomes, since automated decisions made by machines should complement rather than fully replace human judgement.\n",
    "\n",
    "`source`: [\"Algorithmic bias detection and mitigation: Best practices and policies to reduce consumer harms\"](https://www.brookings.edu/research/algorithmic-bias-detection-and-mitigation-best-practices-and-policies-to-reduce-consumer-harms/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
